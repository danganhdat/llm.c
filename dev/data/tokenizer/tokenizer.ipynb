{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "閣下\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training tokenizer: 80 batch [00:00, 118.03 batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[449, 495, 279, 315, 13764, 770, 883, 1071, 69, 3248, 3367, 355, 513, 506, 12, 991, 497, 759, 828, 1253, 5223, 451, 558, 1129, 83, 602, 76, 12, 4664, 12, 1154, 2458, 6784, 1493, 1662, 12, 550, 593, 639, 1789, 1026, 718, 1936, 777, 1090, 2233, 770, 3079, 1788, 852, 942, 682, 871, 1719, 1471, 3079, 1213, 14, 221, 4276, 251, 19, 16, 2353, 12, 536, 3193, 3355, 1778, 2992, 1273, 1657, 12, 5545, 1254, 796, 3193, 4188, 1778, 2992, 641, 1211, 1699, 884, 14, 5594, 80, 777, 2289, 851, 2285, 1143, 536, 967, 1211, 3364, 1379, 12, 1098, 1185, 1211, 2287, 524, 1786, 720, 1154, 4276, 252, 12, 1018, 787, 14, 221]\n",
      "Theo chuyeen gia kinh tếe Hồ Quốc Tuaấn, Giảng viên cao cấp Đại học Bristol, Anh, giới trẻ lười kết hôn, sinh con một phần đến từ áp lực ngày càng gia tăng khi giá bất động sản liên tục tăng lên. “30 tuổi, không đủ giàu để mua chung cư, nhưng cũng chẳng đủ nghèo để mua nhà ở xã hội. Áp lực này là vấn đề không chỉ ở Việt Nam, mà còn ở nhiều nước trên thế giới”, ông nói. \n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "dictionary_url = (\"https://raw.githubusercontent.com/undertheseanlp/dictionary/master/dictionary/words.txt\")\n",
    "tokenizer_dataset = load_dataset(\"json\", data_files=dictionary_url, split=\"train\")\n",
    "\n",
    "print(tokenizer_dataset[-1][\"text\"])\n",
    "\n",
    "from tokenizers import decoders, models, normalizers, pre_tokenizers, processors, trainers, Tokenizer\n",
    "from tokenizers.normalizers import Replace, Lowercase, NFC\n",
    "from tokenizers.pre_tokenizers import Whitespace, Metaspace\n",
    "from tokenizers.models import BPE, WordLevel, Unigram\n",
    "from tokenizers.trainers import BpeTrainer, WordLevelTrainer, UnigramTrainer\n",
    "\n",
    "# tokenizer = Tokenizer(WordLevel(unk_token=\"<|unk|>\"))\n",
    "tokenizer = Tokenizer(BPE())\n",
    "\n",
    "# Normalization Form C (NFC): Characters are decomposed and then re-composed by canonical equivalence\n",
    "tokenizer.normalizer = normalizers.NFC() # \"â\" (U+00E2) -> \"a\" (U+0061) + \"^\" (U+0302) -> \"â\" (U+00E2)\n",
    "\n",
    "# Using a pre-tokenizer will ensure no token is bigger than a word returned by the pre-tokenizer.\n",
    "# tokenizer.pre_tokenizer = Whitespace()\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "\n",
    "# The most important part is to give the special_tokens we plan to use later on.\n",
    "# trainer = WordLevelTrainer(special_tokens=[\"<|unk|>\", \"<|endoftext|>\"], vocab_size=200_000)\n",
    "trainer = BpeTrainer(special_tokens=[\"<|endoftext|>\"], initial_alphabet=pre_tokenizers.ByteLevel.alphabet()) # vocab_size= 50_000, limit_alphabet = 256, min_frequency=2,\n",
    "\n",
    "# The API to train our tokenizer will require an iterator of batch of texts, for instance a list of list of texts:\n",
    "\n",
    "# To avoid loading everything into memory, we define a Python iterator.\n",
    "def batch_iterator(batch_size=1_000, field=\"text\"):\n",
    "    for i in range(0, len(tokenizer_dataset), batch_size):\n",
    "        yield tokenizer_dataset[i : i + batch_size][field]\n",
    "        \n",
    "from tqdm import tqdm\n",
    "iterator = tqdm(batch_iterator(), desc=\"Training tokenizer\", unit=\" batch\")\n",
    "\n",
    "''' WARNING '''\n",
    "tokenizer.train_from_iterator(iterator, trainer=trainer)\n",
    "''' WARNING '''\n",
    "\n",
    "# from tokenizers.processors import TemplateProcessing\n",
    "# tokenizer.post_processor = TemplateProcessing(\n",
    "#     single=\"$A <|endoftext|>\",\n",
    "#     pair=\"$A <|endoftext|> $B:1 <|endoftext|>:1\",\n",
    "#     special_tokens=[\n",
    "#         (\"<|endoftext|>\", tokenizer.token_to_id(\"<|endoftext|>\")),\n",
    "#     ],\n",
    "# )\n",
    "\n",
    "tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "new_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)\n",
    "\n",
    "new_tokenizer.save_pretrained(\"/llm.c/dev/data/bpe_tokenizer_20k_gpt-2\")\n",
    "\n",
    "main_tokenizer = GPT2TokenizerFast.from_pretrained(\"/llm.c/dev/data/bpe_tokenizer_20k_gpt-2\")\n",
    "\n",
    "output = main_tokenizer.encode(\"Theo chuyeen gia kinh tếe Hồ Quốc Tuaấn, Giảng viên cao cấp Đại học Bristol, Anh, \"\n",
    "                              \"giới trẻ lười kết hôn, sinh con một phần đến từ áp lực ngày càng gia tăng khi giá bất động sản liên tục tăng lên. \"\n",
    "                              \"“30 tuổi, không đủ giàu để mua chung cư, nhưng cũng chẳng đủ nghèo để mua nhà ở xã hội. \"\n",
    "                              \"Áp lực này là vấn đề không chỉ ở Việt Nam, mà còn ở nhiều nước trên thế giới”, ông nói. \")\n",
    "\n",
    "print(output)\n",
    "\n",
    "print(main_tokenizer.decode(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\AD/.cache/huggingface/datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19664\\3436019665.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Remove the cache directory and all its contents\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mshutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\shutil.py\u001b[0m in \u001b[0;36mrmtree\u001b[1;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[0;32m    748\u001b[0m             \u001b[1;31m# can't continue even if onerror hook returns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    749\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 750\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_rmtree_unsafe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0monerror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    751\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    752\u001b[0m \u001b[1;31m# Allow introspection of whether or not the hardening against symlink\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\shutil.py\u001b[0m in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    599\u001b[0m             \u001b[0mentries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscandir_it\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 601\u001b[1;33m         \u001b[0monerror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    602\u001b[0m         \u001b[0mentries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    603\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\shutil.py\u001b[0m in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_rmtree_unsafe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0monerror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mscandir_it\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    599\u001b[0m             \u001b[0mentries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscandir_it\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\AD/.cache/huggingface/datasets'"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Get the default cache directory\n",
    "cache_dir = os.path.expanduser(\"~/.cache/huggingface/datasets\")\n",
    "\n",
    "# Remove the cache directory and all its contents\n",
    "shutil.rmtree(cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPUs in the system: 12\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    " \n",
    "cpuCount = os.cpu_count()\n",
    "print(\"Number of CPUs in the system:\", cpuCount)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
